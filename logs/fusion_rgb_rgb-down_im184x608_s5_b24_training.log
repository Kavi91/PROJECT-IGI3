2025-04-25 00:41:40,400 - INFO - Training trajectories: [('Kite_training/sunny', 'trajectory_0000'), ('Kite_training/sunny', 'trajectory_0001'), ('Kite_training/sunny', 'trajectory_0002'), ('Kite_training/sunny', 'trajectory_0003'), ('Kite_training/sunny', 'trajectory_0004'), ('Kite_training/sunny', 'trajectory_0005')], Count: 6
2025-04-25 00:41:40,400 - INFO - Validation trajectories: [('Kite_training/sunny', 'trajectory_0006'), ('Kite_training/sunny', 'trajectory_0007'), ('Kite_training/sunny', 'trajectory_0008'), ('Kite_training/sunny', 'trajectory_0009')], Count: 4
2025-04-25 00:41:40,489 - INFO - Using existing data info files with 10 trajectories
2025-04-25 00:41:40,490 - INFO - Number of training samples: 7958
2025-04-25 00:41:40,490 - INFO - Number of validation samples: 2204
2025-04-25 00:41:40,664 - INFO - Number of training batches: 330
2025-04-25 00:41:40,664 - INFO - Number of validation batches: 91
2025-04-25 00:41:40,664 - INFO - Instantiating FusionVO model
2025-04-25 00:41:43,945 - INFO - Model has 385 parameters
2025-04-25 00:41:43,945 - INFO - Sample model keys: ['rgb_encoder.conv1.0.weight', 'rgb_encoder.conv1.1.weight', 'rgb_encoder.conv1.1.bias', 'rgb_encoder.conv1.1.running_mean', 'rgb_encoder.conv1.1.running_var', 'rgb_encoder.conv1.1.num_batches_tracked', 'rgb_encoder.conv2.0.weight', 'rgb_encoder.conv2.1.weight', 'rgb_encoder.conv2.1.bias', 'rgb_encoder.conv2.1.running_mean']
2025-04-25 00:41:43,945 - INFO - Target prefixes that exist in model: []
2025-04-25 00:41:43,945 - INFO - Extended target prefixes: ['rgb_encoder.flownet', 'rgb_down_encoder.flownet', 'rgb_encoder.conv1', 'rgb_encoder.conv2', 'rgb_encoder.conv3', 'rgb_encoder.conv3_1', 'rgb_encoder.conv4', 'rgb_encoder.conv4_1', 'rgb_encoder.conv5', 'rgb_encoder.conv5_1', 'rgb_encoder.conv6', 'rgb_down_encoder.conv1', 'rgb_down_encoder.conv2', 'rgb_down_encoder.conv3', 'rgb_down_encoder.conv3_1', 'rgb_down_encoder.conv4', 'rgb_down_encoder.conv4_1', 'rgb_down_encoder.conv5', 'rgb_down_encoder.conv5_1', 'rgb_down_encoder.conv6', 'rgb_encoder_flownet', 'rgb_down_encoder_flownet']
2025-04-25 00:41:43,945 - INFO - Loading pre-trained FlowNet weights from /home/krkavinda/DeepVO-pytorch/FlowNet_models/pytorch/flownets_bn_EPE2.459.pth
2025-04-25 00:41:44,010 - INFO - FlowNet weights file contains 4 keys: ['state_dict', 'epoch', 'arch', 'best_EPE']
2025-04-25 00:41:44,010 - INFO - Found 'state_dict' key in weights file, using it for loading
2025-04-25 00:41:44,010 - INFO - Pretrained weights contain 63 parameters
2025-04-25 00:41:44,010 - INFO - Sample weight keys: ['conv1.0.weight', 'conv1.1.weight', 'conv1.1.bias', 'conv1.1.running_mean', 'conv1.1.running_var']
2025-04-25 00:41:44,012 - INFO - No key matches found, attempting shape-based matching...
2025-04-25 00:41:44,012 - INFO - Matched 49 layers by shape
2025-04-25 00:41:44,015 - INFO - Successfully loaded 49 layers from FlowNet weights
2025-04-25 00:41:44,015 - INFO - Starting training for 100 epochs
2025-04-25 00:46:38,612 - INFO - Epoch 1 - ATE: 0.9080 ± 0.8885 m
2025-04-25 00:46:38,612 - INFO - Epoch 1 - RPE Translation: 0.7325 ± 0.7412 m
2025-04-25 00:46:38,612 - INFO - Epoch 1 - RPE Rotation: 0.0123 ± 0.0142 rad
2025-04-25 00:46:38,612 - INFO - Epoch 1/100, Train Loss: 0.076999, Valid Loss: 0.076912, ETA: 8:06:05
2025-04-25 00:46:40,879 - INFO - New best validation loss: 0.076912
2025-04-25 00:46:43,097 - INFO - New best training loss: 0.076999
2025-04-25 00:50:43,246 - INFO - Epoch 2 - ATE: 0.9260 ± 0.8894 m
2025-04-25 00:50:43,246 - INFO - Epoch 2 - RPE Translation: 0.7385 ± 0.7409 m
2025-04-25 00:50:43,246 - INFO - Epoch 2 - RPE Rotation: 0.0111 ± 0.0137 rad
2025-04-25 00:50:43,246 - INFO - Epoch 2/100, Train Loss: 0.075242, Valid Loss: 0.071149, ETA: 7:16:42
2025-04-25 00:50:46,094 - INFO - New best validation loss: 0.071149
2025-04-25 00:50:48,985 - INFO - New best training loss: 0.075242
2025-04-25 00:54:51,326 - INFO - Epoch 3 - ATE: 0.9111 ± 0.8861 m
2025-04-25 00:54:51,326 - INFO - Epoch 3 - RPE Translation: 0.7340 ± 0.7391 m
2025-04-25 00:54:51,326 - INFO - Epoch 3 - RPE Rotation: 0.0106 ± 0.0135 rad
2025-04-25 00:54:51,326 - INFO - Epoch 3/100, Train Loss: 0.075080, Valid Loss: 0.079340, ETA: 6:58:45
2025-04-25 00:54:54,151 - INFO - New best training loss: 0.075080
2025-04-25 00:58:57,180 - INFO - Epoch 4 - ATE: 0.9192 ± 0.8873 m
2025-04-25 00:58:57,181 - INFO - Epoch 4 - RPE Translation: 0.7364 ± 0.7391 m
2025-04-25 00:58:57,181 - INFO - Epoch 4 - RPE Rotation: 0.0109 ± 0.0136 rad
2025-04-25 00:58:57,181 - INFO - Epoch 4/100, Train Loss: 0.075112, Valid Loss: 0.075111, ETA: 6:48:02
2025-04-25 01:02:53,372 - INFO - Epoch 5 - ATE: 0.9274 ± 0.8913 m
2025-04-25 01:02:53,372 - INFO - Epoch 5 - RPE Translation: 0.7378 ± 0.7419 m
2025-04-25 01:02:53,372 - INFO - Epoch 5 - RPE Rotation: 0.0114 ± 0.0138 rad
2025-04-25 01:03:09,620 - INFO - Epoch 5/100, Train Loss: 0.075043, Valid Loss: 0.071354, ETA: 6:42:58
2025-04-25 01:03:12,494 - INFO - New best training loss: 0.075043
2025-04-25 01:03:14,826 - INFO - Saved checkpoint at epoch 5
2025-04-25 03:09:36,961 - INFO - Training trajectories: [('Kite_training/sunny', 'trajectory_0000'), ('Kite_training/sunny', 'trajectory_0001'), ('Kite_training/sunny', 'trajectory_0002'), ('Kite_training/sunny', 'trajectory_0003'), ('Kite_training/sunny', 'trajectory_0004'), ('Kite_training/sunny', 'trajectory_0005')], Count: 6
2025-04-25 03:09:36,961 - INFO - Validation trajectories: [('Kite_training/sunny', 'trajectory_0006'), ('Kite_training/sunny', 'trajectory_0007'), ('Kite_training/sunny', 'trajectory_0008'), ('Kite_training/sunny', 'trajectory_0009')], Count: 4
2025-04-25 03:09:37,054 - INFO - Using existing data info files with 10 trajectories
2025-04-25 03:09:37,054 - INFO - Number of training samples: 7958
2025-04-25 03:09:37,054 - INFO - Number of validation samples: 2204
2025-04-25 03:10:40,332 - INFO - Training trajectories: [('Kite_training/sunny', 'trajectory_0000'), ('Kite_training/sunny', 'trajectory_0001'), ('Kite_training/sunny', 'trajectory_0002'), ('Kite_training/sunny', 'trajectory_0003'), ('Kite_training/sunny', 'trajectory_0004'), ('Kite_training/sunny', 'trajectory_0005')], Count: 6
2025-04-25 03:10:40,332 - INFO - Validation trajectories: [('Kite_training/sunny', 'trajectory_0006'), ('Kite_training/sunny', 'trajectory_0007'), ('Kite_training/sunny', 'trajectory_0008'), ('Kite_training/sunny', 'trajectory_0009')], Count: 4
2025-04-25 03:10:40,423 - INFO - Using existing data info files with 10 trajectories
2025-04-25 03:10:40,423 - INFO - Number of training samples: 7958
2025-04-25 03:10:40,423 - INFO - Number of validation samples: 2204
2025-04-25 03:10:40,565 - INFO - Number of training batches: 330
2025-04-25 03:10:40,565 - INFO - Number of validation batches: 91
2025-04-25 03:10:40,565 - INFO - Instantiating FusionVO model
2025-04-25 03:10:43,622 - INFO - Model has 373 parameters
2025-04-25 03:10:43,622 - INFO - Sample model keys: ['rgb_encoder.conv1.0.weight', 'rgb_encoder.conv1.1.weight', 'rgb_encoder.conv1.1.bias', 'rgb_encoder.conv1.1.running_mean', 'rgb_encoder.conv1.1.running_var', 'rgb_encoder.conv1.1.num_batches_tracked', 'rgb_encoder.conv2.0.weight', 'rgb_encoder.conv2.1.weight', 'rgb_encoder.conv2.1.bias', 'rgb_encoder.conv2.1.running_mean']
2025-04-25 03:10:43,622 - INFO - Target prefixes that exist in model: []
2025-04-25 03:10:43,622 - INFO - Extended target prefixes: ['rgb_encoder.flownet', 'rgb_down_encoder.flownet', 'rgb_encoder.conv1', 'rgb_encoder.conv2', 'rgb_encoder.conv3', 'rgb_encoder.conv3_1', 'rgb_encoder.conv4', 'rgb_encoder.conv4_1', 'rgb_encoder.conv5', 'rgb_encoder.conv5_1', 'rgb_encoder.conv6', 'rgb_down_encoder.conv1', 'rgb_down_encoder.conv2', 'rgb_down_encoder.conv3', 'rgb_down_encoder.conv3_1', 'rgb_down_encoder.conv4', 'rgb_down_encoder.conv4_1', 'rgb_down_encoder.conv5', 'rgb_down_encoder.conv5_1', 'rgb_down_encoder.conv6', 'rgb_encoder_flownet', 'rgb_down_encoder_flownet']
2025-04-25 03:10:43,622 - INFO - Loading pre-trained FlowNet weights from /home/krkavinda/DeepVO-pytorch/FlowNet_models/pytorch/flownets_bn_EPE2.459.pth
2025-04-25 03:10:43,693 - INFO - FlowNet weights file contains 4 keys: ['state_dict', 'epoch', 'arch', 'best_EPE']
2025-04-25 03:10:43,694 - INFO - Found 'state_dict' key in weights file, using it for loading
2025-04-25 03:10:43,694 - INFO - Pretrained weights contain 63 parameters
2025-04-25 03:10:43,694 - INFO - Sample weight keys: ['conv1.0.weight', 'conv1.1.weight', 'conv1.1.bias', 'conv1.1.running_mean', 'conv1.1.running_var']
2025-04-25 03:10:43,695 - INFO - No key matches found, attempting shape-based matching...
2025-04-25 03:10:43,696 - INFO - Matched 49 layers by shape
2025-04-25 03:10:43,698 - INFO - Successfully loaded 49 layers from FlowNet weights
2025-04-25 03:10:43,699 - INFO - Starting training for 100 epochs
2025-04-25 03:17:46,139 - INFO - Epoch 1/100, Train Loss: 0.168295, Valid Loss: 0.063496, ETA: 11:37:01
2025-04-25 03:17:48,379 - INFO - New best validation loss: 0.063496
2025-04-25 03:17:50,508 - INFO - New best training loss: 0.168295
2025-04-25 03:24:05,606 - INFO - Epoch 2/100, Train Loss: 0.088817, Valid Loss: 0.067043, ETA: 10:51:19
2025-04-25 03:24:08,320 - INFO - New best training loss: 0.088817
2025-04-25 03:30:19,182 - INFO - Epoch 3/100, Train Loss: 0.086507, Valid Loss: 0.068402, ETA: 10:29:38
2025-04-25 03:30:21,889 - INFO - New best training loss: 0.086507
2025-04-25 09:18:20,190 - INFO - Training trajectories: [('Kite_training/sunny', 'trajectory_0000'), ('Kite_training/sunny', 'trajectory_0001'), ('Kite_training/sunny', 'trajectory_0002'), ('Kite_training/sunny', 'trajectory_0003'), ('Kite_training/sunny', 'trajectory_0004'), ('Kite_training/sunny', 'trajectory_0005'), ('Kite_training/sunny', 'trajectory_0006'), ('Kite_training/sunny', 'trajectory_0007')], Count: 8
2025-04-25 09:18:20,190 - INFO - Validation trajectories: [('Kite_training/sunny', 'trajectory_0008'), ('Kite_training/sunny', 'trajectory_0009'), ('Kite_training/sunny', 'trajectory_0010'), ('Kite_training/sunny', 'trajectory_0011'), ('Kite_training/sunny', 'trajectory_0012'), ('Kite_training/sunny', 'trajectory_0013'), ('Kite_training/sunny', 'trajectory_0014')], Count: 7
2025-04-25 09:18:20,295 - INFO - Using existing data info files with 15 trajectories
2025-04-25 09:18:20,295 - INFO - Number of training samples: 10605
2025-04-25 09:18:20,295 - INFO - Number of validation samples: 3860
2025-04-25 09:18:20,498 - INFO - Number of training batches: 440
2025-04-25 09:18:20,498 - INFO - Number of validation batches: 160
2025-04-25 09:18:20,498 - INFO - Instantiating FusionVO model
2025-04-25 09:18:22,829 - INFO - Model has 321 parameters
2025-04-25 09:18:22,829 - INFO - Sample model keys: ['rgb_encoder.conv1.0.weight', 'rgb_encoder.conv1.1.weight', 'rgb_encoder.conv1.1.bias', 'rgb_encoder.conv1.1.running_mean', 'rgb_encoder.conv1.1.running_var', 'rgb_encoder.conv1.1.num_batches_tracked', 'rgb_encoder.conv2.0.weight', 'rgb_encoder.conv2.1.weight', 'rgb_encoder.conv2.1.bias', 'rgb_encoder.conv2.1.running_mean']
2025-04-25 09:18:22,829 - INFO - Target prefixes that exist in model: []
2025-04-25 09:18:22,829 - INFO - Extended target prefixes: ['rgb_encoder.flownet', 'rgb_down_encoder.flownet', 'rgb_encoder.conv1', 'rgb_encoder.conv2', 'rgb_encoder.conv3', 'rgb_encoder.conv3_1', 'rgb_encoder.conv4', 'rgb_encoder.conv4_1', 'rgb_encoder.conv5', 'rgb_encoder.conv5_1', 'rgb_encoder.conv6', 'rgb_down_encoder.conv1', 'rgb_down_encoder.conv2', 'rgb_down_encoder.conv3', 'rgb_down_encoder.conv3_1', 'rgb_down_encoder.conv4', 'rgb_down_encoder.conv4_1', 'rgb_down_encoder.conv5', 'rgb_down_encoder.conv5_1', 'rgb_down_encoder.conv6', 'rgb_encoder_flownet', 'rgb_down_encoder_flownet']
2025-04-25 09:18:22,829 - INFO - Loading pre-trained FlowNet weights from /home/krkavinda/DeepVO-pytorch/FlowNet_models/pytorch/flownets_bn_EPE2.459.pth
2025-04-25 09:18:22,913 - INFO - FlowNet weights file contains 4 keys: ['state_dict', 'epoch', 'arch', 'best_EPE']
2025-04-25 09:18:22,914 - INFO - Found 'state_dict' key in weights file, using it for loading
2025-04-25 09:18:22,914 - INFO - Pretrained weights contain 63 parameters
2025-04-25 09:18:22,914 - INFO - Sample weight keys: ['conv1.0.weight', 'conv1.1.weight', 'conv1.1.bias', 'conv1.1.running_mean', 'conv1.1.running_var']
2025-04-25 09:18:22,915 - INFO - No key matches found, attempting shape-based matching...
2025-04-25 09:18:22,915 - INFO - Matched 49 layers by shape
2025-04-25 09:18:22,918 - INFO - Successfully loaded 49 layers from FlowNet weights
2025-04-25 09:18:22,918 - INFO - Starting training for 100 epochs
2025-04-25 09:24:32,480 - INFO - Training trajectories: [('Kite_training/sunny', 'trajectory_0000'), ('Kite_training/sunny', 'trajectory_0001'), ('Kite_training/sunny', 'trajectory_0002'), ('Kite_training/sunny', 'trajectory_0003'), ('Kite_training/sunny', 'trajectory_0004'), ('Kite_training/sunny', 'trajectory_0005'), ('Kite_training/sunny', 'trajectory_0006'), ('Kite_training/sunny', 'trajectory_0007')], Count: 8
2025-04-25 09:24:32,480 - INFO - Validation trajectories: [('Kite_training/sunny', 'trajectory_0008'), ('Kite_training/sunny', 'trajectory_0009'), ('Kite_training/sunny', 'trajectory_0010'), ('Kite_training/sunny', 'trajectory_0011'), ('Kite_training/sunny', 'trajectory_0012'), ('Kite_training/sunny', 'trajectory_0013'), ('Kite_training/sunny', 'trajectory_0014')], Count: 7
2025-04-25 09:24:32,582 - INFO - Using existing data info files with 15 trajectories
2025-04-25 09:24:32,582 - INFO - Number of training samples: 10605
2025-04-25 09:24:32,582 - INFO - Number of validation samples: 3860
2025-04-25 09:24:32,786 - INFO - Number of training batches: 440
2025-04-25 09:24:32,786 - INFO - Number of validation batches: 160
2025-04-25 09:24:32,786 - INFO - Instantiating FusionVO model
2025-04-25 09:24:35,157 - INFO - Model has 323 parameters
2025-04-25 09:24:35,157 - INFO - Sample model keys: ['rgb_encoder.conv1.0.weight', 'rgb_encoder.conv1.1.weight', 'rgb_encoder.conv1.1.bias', 'rgb_encoder.conv1.1.running_mean', 'rgb_encoder.conv1.1.running_var', 'rgb_encoder.conv1.1.num_batches_tracked', 'rgb_encoder.conv2.0.weight', 'rgb_encoder.conv2.1.weight', 'rgb_encoder.conv2.1.bias', 'rgb_encoder.conv2.1.running_mean']
2025-04-25 09:24:35,157 - INFO - Target prefixes that exist in model: []
2025-04-25 09:24:35,157 - INFO - Extended target prefixes: ['rgb_encoder.flownet', 'rgb_down_encoder.flownet', 'rgb_encoder.conv1', 'rgb_encoder.conv2', 'rgb_encoder.conv3', 'rgb_encoder.conv3_1', 'rgb_encoder.conv4', 'rgb_encoder.conv4_1', 'rgb_encoder.conv5', 'rgb_encoder.conv5_1', 'rgb_encoder.conv6', 'rgb_down_encoder.conv1', 'rgb_down_encoder.conv2', 'rgb_down_encoder.conv3', 'rgb_down_encoder.conv3_1', 'rgb_down_encoder.conv4', 'rgb_down_encoder.conv4_1', 'rgb_down_encoder.conv5', 'rgb_down_encoder.conv5_1', 'rgb_down_encoder.conv6', 'rgb_encoder_flownet', 'rgb_down_encoder_flownet']
2025-04-25 09:24:35,157 - INFO - Loading pre-trained FlowNet weights from /home/krkavinda/DeepVO-pytorch/FlowNet_models/pytorch/flownets_bn_EPE2.459.pth
2025-04-25 09:24:35,226 - INFO - FlowNet weights file contains 4 keys: ['state_dict', 'epoch', 'arch', 'best_EPE']
2025-04-25 09:24:35,226 - INFO - Found 'state_dict' key in weights file, using it for loading
2025-04-25 09:24:35,226 - INFO - Pretrained weights contain 63 parameters
2025-04-25 09:24:35,226 - INFO - Sample weight keys: ['conv1.0.weight', 'conv1.1.weight', 'conv1.1.bias', 'conv1.1.running_mean', 'conv1.1.running_var']
2025-04-25 09:24:35,227 - INFO - No key matches found, attempting shape-based matching...
2025-04-25 09:24:35,228 - INFO - Matched 49 layers by shape
2025-04-25 09:24:35,230 - INFO - Successfully loaded 49 layers from FlowNet weights
2025-04-25 09:24:35,231 - INFO - Starting training for 100 epochs
2025-04-25 09:27:02,072 - INFO - Training trajectories: [('Kite_training/sunny', 'trajectory_0000'), ('Kite_training/sunny', 'trajectory_0001'), ('Kite_training/sunny', 'trajectory_0002'), ('Kite_training/sunny', 'trajectory_0003'), ('Kite_training/sunny', 'trajectory_0004'), ('Kite_training/sunny', 'trajectory_0005'), ('Kite_training/sunny', 'trajectory_0006'), ('Kite_training/sunny', 'trajectory_0007')], Count: 8
2025-04-25 09:27:02,072 - INFO - Validation trajectories: [('Kite_training/sunny', 'trajectory_0008'), ('Kite_training/sunny', 'trajectory_0009'), ('Kite_training/sunny', 'trajectory_0010'), ('Kite_training/sunny', 'trajectory_0011'), ('Kite_training/sunny', 'trajectory_0012'), ('Kite_training/sunny', 'trajectory_0013'), ('Kite_training/sunny', 'trajectory_0014')], Count: 7
2025-04-25 09:27:02,173 - INFO - Using existing data info files with 15 trajectories
2025-04-25 09:27:02,173 - INFO - Number of training samples: 10605
2025-04-25 09:27:02,173 - INFO - Number of validation samples: 3860
2025-04-25 09:27:02,374 - INFO - Number of training batches: 440
2025-04-25 09:27:02,374 - INFO - Number of validation batches: 160
2025-04-25 09:27:02,374 - INFO - Instantiating FusionVO model
2025-04-25 09:27:04,687 - INFO - Model has 323 parameters
2025-04-25 09:27:04,687 - INFO - Sample model keys: ['rgb_encoder.conv1.0.weight', 'rgb_encoder.conv1.1.weight', 'rgb_encoder.conv1.1.bias', 'rgb_encoder.conv1.1.running_mean', 'rgb_encoder.conv1.1.running_var', 'rgb_encoder.conv1.1.num_batches_tracked', 'rgb_encoder.conv2.0.weight', 'rgb_encoder.conv2.1.weight', 'rgb_encoder.conv2.1.bias', 'rgb_encoder.conv2.1.running_mean']
2025-04-25 09:27:04,687 - INFO - Target prefixes that exist in model: []
2025-04-25 09:27:04,687 - INFO - Extended target prefixes: ['rgb_encoder.flownet', 'rgb_down_encoder.flownet', 'rgb_encoder.conv1', 'rgb_encoder.conv2', 'rgb_encoder.conv3', 'rgb_encoder.conv3_1', 'rgb_encoder.conv4', 'rgb_encoder.conv4_1', 'rgb_encoder.conv5', 'rgb_encoder.conv5_1', 'rgb_encoder.conv6', 'rgb_down_encoder.conv1', 'rgb_down_encoder.conv2', 'rgb_down_encoder.conv3', 'rgb_down_encoder.conv3_1', 'rgb_down_encoder.conv4', 'rgb_down_encoder.conv4_1', 'rgb_down_encoder.conv5', 'rgb_down_encoder.conv5_1', 'rgb_down_encoder.conv6', 'rgb_encoder_flownet', 'rgb_down_encoder_flownet']
2025-04-25 09:27:04,687 - INFO - Loading pre-trained FlowNet weights from /home/krkavinda/DeepVO-pytorch/FlowNet_models/pytorch/flownets_bn_EPE2.459.pth
2025-04-25 09:27:04,752 - INFO - FlowNet weights file contains 4 keys: ['state_dict', 'epoch', 'arch', 'best_EPE']
2025-04-25 09:27:04,752 - INFO - Found 'state_dict' key in weights file, using it for loading
2025-04-25 09:27:04,752 - INFO - Pretrained weights contain 63 parameters
2025-04-25 09:27:04,752 - INFO - Sample weight keys: ['conv1.0.weight', 'conv1.1.weight', 'conv1.1.bias', 'conv1.1.running_mean', 'conv1.1.running_var']
2025-04-25 09:27:04,753 - INFO - No key matches found, attempting shape-based matching...
2025-04-25 09:27:04,754 - INFO - Matched 49 layers by shape
2025-04-25 09:27:04,756 - INFO - Successfully loaded 49 layers from FlowNet weights
2025-04-25 09:27:04,757 - INFO - Starting training for 100 epochs
2025-04-25 09:30:20,662 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:20,767 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:20,869 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:21,395 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:21,496 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:21,598 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:21,692 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:21,794 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:22,093 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:22,207 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:22,319 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:22,437 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:22,554 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:22,663 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:22,786 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:22,896 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:23,010 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:23,128 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:23,244 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:23,353 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:23,458 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:23,583 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:23,689 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:23,790 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:23,892 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:27,393 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:28,140 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:28,233 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:28,324 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:28,416 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:28,515 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:28,614 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:28,711 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:28,815 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:28,915 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:29,018 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:29,122 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:29,238 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:29,351 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:29,470 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:29,589 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:29,699 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:29,823 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:29,950 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:30,071 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:30,178 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:30,281 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:30,390 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:30,506 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:30,613 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:34,697 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:35,344 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:35,442 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:35,542 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:35,638 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:35,737 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:35,844 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:35,947 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:36,057 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:36,166 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:36,278 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:36,388 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:36,494 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:36,614 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:36,734 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:36,845 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:36,954 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:37,079 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:37,201 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:37,313 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:37,417 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:37,525 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:37,632 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:37,735 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:37,834 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:43,629 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:43,903 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:43,983 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:44,062 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:44,143 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:44,222 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:44,301 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:44,382 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:44,462 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:44,541 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:44,623 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:44,708 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:44,796 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:44,880 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:44,965 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:45,056 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:45,147 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:45,236 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:45,331 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:45,429 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:45,519 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:45,621 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:45,724 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:45,827 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:45,932 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:47,055 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:47,151 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:47,254 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:47,349 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:47,440 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:47,534 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:47,626 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:47,722 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:47,819 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:47,916 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:48,009 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:48,105 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:48,198 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:48,290 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:48,386 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:48,481 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:48,578 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:48,674 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:48,768 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:48,863 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:48,958 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:49,056 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:49,155 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:49,253 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:49,350 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:51,176 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:51,280 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:51,377 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:51,469 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:51,567 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:51,830 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:52,120 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:52,657 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:52,735 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:52,816 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:52,898 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:52,980 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:53,060 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:53,139 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:53,219 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:53,303 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:53,385 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:53,465 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:53,544 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:53,669 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:53,748 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:53,829 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:53,908 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:53,986 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:54,067 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:54,907 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:54,983 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:55,060 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:55,136 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:55,213 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:55,289 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:55,365 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:55,440 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:55,516 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:55,593 - ERROR - Error in validation batch: too many values to unpack (expected 2)
2025-04-25 09:30:55,712 - INFO - Epoch 1/100, Train Loss: -80.495855, Valid Loss: 0.000000, ETA: 6:21:04
2025-04-25 09:30:58,217 - INFO - New best validation loss: 0.000000
2025-04-25 09:31:00,621 - INFO - New best training loss: -80.495855
2025-04-25 10:23:59,983 - INFO - Training trajectories: [('Kite_training/sunny', 'trajectory_0000'), ('Kite_training/sunny', 'trajectory_0001'), ('Kite_training/sunny', 'trajectory_0002'), ('Kite_training/sunny', 'trajectory_0003'), ('Kite_training/sunny', 'trajectory_0004'), ('Kite_training/sunny', 'trajectory_0005'), ('Kite_training/sunny', 'trajectory_0006'), ('Kite_training/sunny', 'trajectory_0007')], Count: 8
2025-04-25 10:23:59,983 - INFO - Validation trajectories: [('Kite_training/sunny', 'trajectory_0008'), ('Kite_training/sunny', 'trajectory_0009'), ('Kite_training/sunny', 'trajectory_0010'), ('Kite_training/sunny', 'trajectory_0011'), ('Kite_training/sunny', 'trajectory_0012'), ('Kite_training/sunny', 'trajectory_0013'), ('Kite_training/sunny', 'trajectory_0014')], Count: 7
2025-04-25 10:24:00,082 - INFO - Using existing data info files with 15 trajectories
2025-04-25 10:24:00,083 - INFO - Number of training samples: 10605
2025-04-25 10:24:00,083 - INFO - Number of validation samples: 3860
2025-04-25 10:24:00,284 - INFO - Number of training batches: 440
2025-04-25 10:24:00,284 - INFO - Number of validation batches: 160
2025-04-25 10:24:00,284 - INFO - Instantiating FusionVO model
2025-04-25 10:24:02,602 - INFO - 
==== MODEL DIAGNOSIS ====
2025-04-25 10:24:02,602 - INFO - Model type: fusion
2025-04-25 10:24:02,602 - INFO - Model class: CrossModalFusionVO
2025-04-25 10:24:02,603 - INFO - Total parameters: 407,233,904
2025-04-25 10:24:02,603 - INFO - Trainable parameters: 407,233,904
2025-04-25 10:24:02,607 - DEBUG - rgb_encoder.conv1.0.weight: shape=torch.Size([64, 6, 7, 7]), mean=0.0006, std=0.0820
2025-04-25 10:24:02,613 - DEBUG - rgb_encoder.conv1.1.weight: shape=torch.Size([64]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,613 - DEBUG - rgb_encoder.conv1.1.bias: shape=torch.Size([64]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,614 - DEBUG - rgb_encoder.conv2.0.weight: shape=torch.Size([128, 64, 5, 5]), mean=0.0001, std=0.0353
2025-04-25 10:24:02,614 - DEBUG - rgb_encoder.conv2.1.weight: shape=torch.Size([128]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,614 - DEBUG - rgb_encoder.conv2.1.bias: shape=torch.Size([128]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,614 - DEBUG - rgb_encoder.conv3.0.weight: shape=torch.Size([256, 128, 5, 5]), mean=0.0000, std=0.0250
2025-04-25 10:24:02,614 - DEBUG - rgb_encoder.conv3.1.weight: shape=torch.Size([256]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,614 - DEBUG - rgb_encoder.conv3.1.bias: shape=torch.Size([256]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,615 - DEBUG - rgb_encoder.conv3_1.0.weight: shape=torch.Size([256, 256, 3, 3]), mean=0.0000, std=0.0295
2025-04-25 10:24:02,615 - DEBUG - rgb_encoder.conv3_1.1.weight: shape=torch.Size([256]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,615 - DEBUG - rgb_encoder.conv3_1.1.bias: shape=torch.Size([256]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,615 - DEBUG - rgb_encoder.conv4.0.weight: shape=torch.Size([512, 256, 3, 3]), mean=0.0001, std=0.0295
2025-04-25 10:24:02,615 - DEBUG - rgb_encoder.conv4.1.weight: shape=torch.Size([512]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,615 - DEBUG - rgb_encoder.conv4.1.bias: shape=torch.Size([512]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,615 - DEBUG - rgb_encoder.conv4_1.0.weight: shape=torch.Size([512, 512, 3, 3]), mean=0.0000, std=0.0208
2025-04-25 10:24:02,616 - DEBUG - rgb_encoder.conv4_1.1.weight: shape=torch.Size([512]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,616 - DEBUG - rgb_encoder.conv4_1.1.bias: shape=torch.Size([512]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,616 - DEBUG - rgb_encoder.conv5.0.weight: shape=torch.Size([512, 512, 3, 3]), mean=0.0000, std=0.0208
2025-04-25 10:24:02,616 - DEBUG - rgb_encoder.conv5.1.weight: shape=torch.Size([512]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,616 - DEBUG - rgb_encoder.conv5.1.bias: shape=torch.Size([512]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,616 - DEBUG - rgb_encoder.conv5_1.0.weight: shape=torch.Size([512, 512, 3, 3]), mean=0.0000, std=0.0208
2025-04-25 10:24:02,616 - DEBUG - rgb_encoder.conv5_1.1.weight: shape=torch.Size([512]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,616 - DEBUG - rgb_encoder.conv5_1.1.bias: shape=torch.Size([512]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,617 - DEBUG - rgb_encoder.conv6.0.weight: shape=torch.Size([1024, 512, 3, 3]), mean=-0.0000, std=0.0208
2025-04-25 10:24:02,617 - DEBUG - rgb_encoder.conv6.1.weight: shape=torch.Size([1024]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,617 - DEBUG - rgb_encoder.conv6.1.bias: shape=torch.Size([1024]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,618 - DEBUG - rgb_encoder.rnn.weight_ih_l0: shape=torch.Size([4000, 30720]), mean=0.0000, std=0.0081
2025-04-25 10:24:02,622 - DEBUG - rgb_encoder.rnn.weight_hh_l0: shape=torch.Size([4000, 1000]), mean=0.0000, std=0.0447
2025-04-25 10:24:02,622 - DEBUG - rgb_encoder.rnn.bias_ih_l0: shape=torch.Size([4000]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,622 - DEBUG - rgb_encoder.rnn.bias_hh_l0: shape=torch.Size([4000]), mean=0.2500, std=0.4331
2025-04-25 10:24:02,622 - DEBUG - rgb_encoder.rnn.weight_ih_l1: shape=torch.Size([4000, 1000]), mean=-0.0000, std=0.0447
2025-04-25 10:24:02,622 - DEBUG - rgb_encoder.rnn.weight_hh_l1: shape=torch.Size([4000, 1000]), mean=0.0000, std=0.0447
2025-04-25 10:24:02,623 - DEBUG - rgb_encoder.rnn.bias_ih_l1: shape=torch.Size([4000]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,623 - DEBUG - rgb_encoder.rnn.bias_hh_l1: shape=torch.Size([4000]), mean=0.2500, std=0.4331
2025-04-25 10:24:02,623 - DEBUG - rgb_encoder.linear.weight: shape=torch.Size([6, 1000]), mean=-0.0006, std=0.0448
2025-04-25 10:24:02,623 - DEBUG - rgb_encoder.linear.bias: shape=torch.Size([6]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,623 - DEBUG - rgb_down_encoder.conv1.0.weight: shape=torch.Size([64, 6, 7, 7]), mean=0.0002, std=0.0824
2025-04-25 10:24:02,623 - DEBUG - rgb_down_encoder.conv1.1.weight: shape=torch.Size([64]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,623 - DEBUG - rgb_down_encoder.conv1.1.bias: shape=torch.Size([64]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,624 - DEBUG - rgb_down_encoder.conv2.0.weight: shape=torch.Size([128, 64, 5, 5]), mean=-0.0000, std=0.0353
2025-04-25 10:24:02,624 - DEBUG - rgb_down_encoder.conv2.1.weight: shape=torch.Size([128]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,624 - DEBUG - rgb_down_encoder.conv2.1.bias: shape=torch.Size([128]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,624 - DEBUG - rgb_down_encoder.conv3.0.weight: shape=torch.Size([256, 128, 5, 5]), mean=0.0000, std=0.0250
2025-04-25 10:24:02,624 - DEBUG - rgb_down_encoder.conv3.1.weight: shape=torch.Size([256]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,624 - DEBUG - rgb_down_encoder.conv3.1.bias: shape=torch.Size([256]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,624 - DEBUG - rgb_down_encoder.conv3_1.0.weight: shape=torch.Size([256, 256, 3, 3]), mean=-0.0001, std=0.0295
2025-04-25 10:24:02,625 - DEBUG - rgb_down_encoder.conv3_1.1.weight: shape=torch.Size([256]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,625 - DEBUG - rgb_down_encoder.conv3_1.1.bias: shape=torch.Size([256]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,625 - DEBUG - rgb_down_encoder.conv4.0.weight: shape=torch.Size([512, 256, 3, 3]), mean=0.0000, std=0.0294
2025-04-25 10:24:02,625 - DEBUG - rgb_down_encoder.conv4.1.weight: shape=torch.Size([512]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,625 - DEBUG - rgb_down_encoder.conv4.1.bias: shape=torch.Size([512]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,625 - DEBUG - rgb_down_encoder.conv4_1.0.weight: shape=torch.Size([512, 512, 3, 3]), mean=0.0000, std=0.0208
2025-04-25 10:24:02,625 - DEBUG - rgb_down_encoder.conv4_1.1.weight: shape=torch.Size([512]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,625 - DEBUG - rgb_down_encoder.conv4_1.1.bias: shape=torch.Size([512]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,626 - DEBUG - rgb_down_encoder.conv5.0.weight: shape=torch.Size([512, 512, 3, 3]), mean=0.0000, std=0.0208
2025-04-25 10:24:02,626 - DEBUG - rgb_down_encoder.conv5.1.weight: shape=torch.Size([512]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,626 - DEBUG - rgb_down_encoder.conv5.1.bias: shape=torch.Size([512]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,626 - DEBUG - rgb_down_encoder.conv5_1.0.weight: shape=torch.Size([512, 512, 3, 3]), mean=0.0000, std=0.0208
2025-04-25 10:24:02,626 - DEBUG - rgb_down_encoder.conv5_1.1.weight: shape=torch.Size([512]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,626 - DEBUG - rgb_down_encoder.conv5_1.1.bias: shape=torch.Size([512]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,626 - DEBUG - rgb_down_encoder.conv6.0.weight: shape=torch.Size([1024, 512, 3, 3]), mean=0.0000, std=0.0208
2025-04-25 10:24:02,627 - DEBUG - rgb_down_encoder.conv6.1.weight: shape=torch.Size([1024]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,627 - DEBUG - rgb_down_encoder.conv6.1.bias: shape=torch.Size([1024]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,628 - DEBUG - rgb_down_encoder.rnn.weight_ih_l0: shape=torch.Size([4000, 30720]), mean=-0.0000, std=0.0081
2025-04-25 10:24:02,632 - DEBUG - rgb_down_encoder.rnn.weight_hh_l0: shape=torch.Size([4000, 1000]), mean=0.0000, std=0.0447
2025-04-25 10:24:02,632 - DEBUG - rgb_down_encoder.rnn.bias_ih_l0: shape=torch.Size([4000]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,632 - DEBUG - rgb_down_encoder.rnn.bias_hh_l0: shape=torch.Size([4000]), mean=0.2500, std=0.4331
2025-04-25 10:24:02,632 - DEBUG - rgb_down_encoder.rnn.weight_ih_l1: shape=torch.Size([4000, 1000]), mean=0.0000, std=0.0447
2025-04-25 10:24:02,632 - DEBUG - rgb_down_encoder.rnn.weight_hh_l1: shape=torch.Size([4000, 1000]), mean=-0.0000, std=0.0447
2025-04-25 10:24:02,632 - DEBUG - rgb_down_encoder.rnn.bias_ih_l1: shape=torch.Size([4000]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,632 - DEBUG - rgb_down_encoder.rnn.bias_hh_l1: shape=torch.Size([4000]), mean=0.2500, std=0.4331
2025-04-25 10:24:02,633 - DEBUG - rgb_down_encoder.linear.weight: shape=torch.Size([6, 1000]), mean=-0.0001, std=0.0442
2025-04-25 10:24:02,633 - DEBUG - rgb_down_encoder.linear.bias: shape=torch.Size([6]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,633 - DEBUG - depth_encoder.resnet.conv1.weight: shape=torch.Size([64, 1, 7, 7]), mean=0.0053, std=0.2065
2025-04-25 10:24:02,633 - DEBUG - depth_encoder.resnet.bn1.weight: shape=torch.Size([64]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,633 - DEBUG - depth_encoder.resnet.bn1.bias: shape=torch.Size([64]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,633 - DEBUG - depth_encoder.resnet.layer1.0.conv1.weight: shape=torch.Size([64, 64, 3, 3]), mean=-0.0001, std=0.0586
2025-04-25 10:24:02,633 - DEBUG - depth_encoder.resnet.layer1.0.bn1.weight: shape=torch.Size([64]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,633 - DEBUG - depth_encoder.resnet.layer1.0.bn1.bias: shape=torch.Size([64]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,634 - DEBUG - depth_encoder.resnet.layer1.0.conv2.weight: shape=torch.Size([64, 64, 3, 3]), mean=0.0004, std=0.0587
2025-04-25 10:24:02,634 - DEBUG - depth_encoder.resnet.layer1.0.bn2.weight: shape=torch.Size([64]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,634 - DEBUG - depth_encoder.resnet.layer1.0.bn2.bias: shape=torch.Size([64]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,634 - DEBUG - depth_encoder.resnet.layer1.1.conv1.weight: shape=torch.Size([64, 64, 3, 3]), mean=0.0003, std=0.0590
2025-04-25 10:24:02,634 - DEBUG - depth_encoder.resnet.layer1.1.bn1.weight: shape=torch.Size([64]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,634 - DEBUG - depth_encoder.resnet.layer1.1.bn1.bias: shape=torch.Size([64]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,634 - DEBUG - depth_encoder.resnet.layer1.1.conv2.weight: shape=torch.Size([64, 64, 3, 3]), mean=-0.0001, std=0.0594
2025-04-25 10:24:02,635 - DEBUG - depth_encoder.resnet.layer1.1.bn2.weight: shape=torch.Size([64]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,635 - DEBUG - depth_encoder.resnet.layer1.1.bn2.bias: shape=torch.Size([64]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,635 - DEBUG - depth_encoder.resnet.layer2.0.conv1.weight: shape=torch.Size([128, 64, 3, 3]), mean=0.0001, std=0.0587
2025-04-25 10:24:02,635 - DEBUG - depth_encoder.resnet.layer2.0.bn1.weight: shape=torch.Size([128]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,635 - DEBUG - depth_encoder.resnet.layer2.0.bn1.bias: shape=torch.Size([128]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,635 - DEBUG - depth_encoder.resnet.layer2.0.conv2.weight: shape=torch.Size([128, 128, 3, 3]), mean=-0.0000, std=0.0417
2025-04-25 10:24:02,635 - DEBUG - depth_encoder.resnet.layer2.0.bn2.weight: shape=torch.Size([128]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,635 - DEBUG - depth_encoder.resnet.layer2.0.bn2.bias: shape=torch.Size([128]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,635 - DEBUG - depth_encoder.resnet.layer2.0.downsample.0.weight: shape=torch.Size([128, 64, 1, 1]), mean=-0.0002, std=0.1751
2025-04-25 10:24:02,636 - DEBUG - depth_encoder.resnet.layer2.0.downsample.1.weight: shape=torch.Size([128]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,636 - DEBUG - depth_encoder.resnet.layer2.0.downsample.1.bias: shape=torch.Size([128]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,636 - DEBUG - depth_encoder.resnet.layer2.1.conv1.weight: shape=torch.Size([128, 128, 3, 3]), mean=0.0001, std=0.0416
2025-04-25 10:24:02,636 - DEBUG - depth_encoder.resnet.layer2.1.bn1.weight: shape=torch.Size([128]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,636 - DEBUG - depth_encoder.resnet.layer2.1.bn1.bias: shape=torch.Size([128]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,636 - DEBUG - depth_encoder.resnet.layer2.1.conv2.weight: shape=torch.Size([128, 128, 3, 3]), mean=-0.0001, std=0.0416
2025-04-25 10:24:02,636 - DEBUG - depth_encoder.resnet.layer2.1.bn2.weight: shape=torch.Size([128]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,636 - DEBUG - depth_encoder.resnet.layer2.1.bn2.bias: shape=torch.Size([128]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,637 - DEBUG - depth_encoder.resnet.layer3.0.conv1.weight: shape=torch.Size([256, 128, 3, 3]), mean=-0.0000, std=0.0417
2025-04-25 10:24:02,637 - DEBUG - depth_encoder.resnet.layer3.0.bn1.weight: shape=torch.Size([256]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,637 - DEBUG - depth_encoder.resnet.layer3.0.bn1.bias: shape=torch.Size([256]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,637 - DEBUG - depth_encoder.resnet.layer3.0.conv2.weight: shape=torch.Size([256, 256, 3, 3]), mean=-0.0000, std=0.0295
2025-04-25 10:24:02,637 - DEBUG - depth_encoder.resnet.layer3.0.bn2.weight: shape=torch.Size([256]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,637 - DEBUG - depth_encoder.resnet.layer3.0.bn2.bias: shape=torch.Size([256]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,637 - DEBUG - depth_encoder.resnet.layer3.0.downsample.0.weight: shape=torch.Size([256, 128, 1, 1]), mean=0.0002, std=0.1256
2025-04-25 10:24:02,637 - DEBUG - depth_encoder.resnet.layer3.0.downsample.1.weight: shape=torch.Size([256]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,638 - DEBUG - depth_encoder.resnet.layer3.0.downsample.1.bias: shape=torch.Size([256]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,638 - DEBUG - depth_encoder.resnet.layer3.1.conv1.weight: shape=torch.Size([256, 256, 3, 3]), mean=-0.0000, std=0.0295
2025-04-25 10:24:02,638 - DEBUG - depth_encoder.resnet.layer3.1.bn1.weight: shape=torch.Size([256]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,638 - DEBUG - depth_encoder.resnet.layer3.1.bn1.bias: shape=torch.Size([256]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,638 - DEBUG - depth_encoder.resnet.layer3.1.conv2.weight: shape=torch.Size([256, 256, 3, 3]), mean=0.0000, std=0.0294
2025-04-25 10:24:02,638 - DEBUG - depth_encoder.resnet.layer3.1.bn2.weight: shape=torch.Size([256]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,638 - DEBUG - depth_encoder.resnet.layer3.1.bn2.bias: shape=torch.Size([256]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,638 - DEBUG - depth_encoder.resnet.layer4.0.conv1.weight: shape=torch.Size([512, 256, 3, 3]), mean=0.0000, std=0.0295
2025-04-25 10:24:02,638 - DEBUG - depth_encoder.resnet.layer4.0.bn1.weight: shape=torch.Size([512]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,639 - DEBUG - depth_encoder.resnet.layer4.0.bn1.bias: shape=torch.Size([512]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,639 - DEBUG - depth_encoder.resnet.layer4.0.conv2.weight: shape=torch.Size([512, 512, 3, 3]), mean=-0.0000, std=0.0208
2025-04-25 10:24:02,639 - DEBUG - depth_encoder.resnet.layer4.0.bn2.weight: shape=torch.Size([512]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,639 - DEBUG - depth_encoder.resnet.layer4.0.bn2.bias: shape=torch.Size([512]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,639 - DEBUG - depth_encoder.resnet.layer4.0.downsample.0.weight: shape=torch.Size([512, 256, 1, 1]), mean=-0.0005, std=0.0884
2025-04-25 10:24:02,639 - DEBUG - depth_encoder.resnet.layer4.0.downsample.1.weight: shape=torch.Size([512]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,639 - DEBUG - depth_encoder.resnet.layer4.0.downsample.1.bias: shape=torch.Size([512]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,640 - DEBUG - depth_encoder.resnet.layer4.1.conv1.weight: shape=torch.Size([512, 512, 3, 3]), mean=0.0000, std=0.0208
2025-04-25 10:24:02,640 - DEBUG - depth_encoder.resnet.layer4.1.bn1.weight: shape=torch.Size([512]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,640 - DEBUG - depth_encoder.resnet.layer4.1.bn1.bias: shape=torch.Size([512]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,640 - DEBUG - depth_encoder.resnet.layer4.1.conv2.weight: shape=torch.Size([512, 512, 3, 3]), mean=0.0000, std=0.0208
2025-04-25 10:24:02,640 - DEBUG - depth_encoder.resnet.layer4.1.bn2.weight: shape=torch.Size([512]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,640 - DEBUG - depth_encoder.resnet.layer4.1.bn2.bias: shape=torch.Size([512]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,641 - DEBUG - rgb_projection.weight: shape=torch.Size([1000, 30720]), mean=0.0000, std=0.0081
2025-04-25 10:24:02,641 - DEBUG - rgb_projection.bias: shape=torch.Size([1000]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,642 - DEBUG - rgb_down_projection.weight: shape=torch.Size([1000, 30720]), mean=-0.0000, std=0.0081
2025-04-25 10:24:02,643 - DEBUG - rgb_down_projection.bias: shape=torch.Size([1000]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,643 - DEBUG - depth_projection.weight: shape=torch.Size([1000, 512]), mean=-0.0000, std=0.0625
2025-04-25 10:24:02,643 - DEBUG - depth_projection.bias: shape=torch.Size([1000]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,643 - DEBUG - rgb_uncertainty.log_var: shape=torch.Size([1000]), mean=-1.0000, std=0.0000
2025-04-25 10:24:02,643 - DEBUG - rgb_down_uncertainty.log_var: shape=torch.Size([1000]), mean=-1.0000, std=0.0000
2025-04-25 10:24:02,643 - DEBUG - depth_uncertainty.log_var: shape=torch.Size([1000]), mean=-1.0000, std=0.0000
2025-04-25 10:24:02,643 - DEBUG - rgb_to_depth_attention.q_proj.weight: shape=torch.Size([1000, 1000]), mean=-0.0000, std=0.0183
2025-04-25 10:24:02,644 - DEBUG - rgb_to_depth_attention.q_proj.bias: shape=torch.Size([1000]), mean=-0.0007, std=0.0178
2025-04-25 10:24:02,644 - DEBUG - rgb_to_depth_attention.k_proj.weight: shape=torch.Size([1000, 1000]), mean=0.0000, std=0.0183
2025-04-25 10:24:02,644 - DEBUG - rgb_to_depth_attention.k_proj.bias: shape=torch.Size([1000]), mean=0.0004, std=0.0179
2025-04-25 10:24:02,644 - DEBUG - rgb_to_depth_attention.v_proj.weight: shape=torch.Size([1000, 1000]), mean=0.0000, std=0.0183
2025-04-25 10:24:02,644 - DEBUG - rgb_to_depth_attention.v_proj.bias: shape=torch.Size([1000]), mean=-0.0004, std=0.0182
2025-04-25 10:24:02,644 - DEBUG - rgb_to_depth_attention.out_proj.weight: shape=torch.Size([1000, 1000]), mean=-0.0000, std=0.0183
2025-04-25 10:24:02,644 - DEBUG - rgb_to_depth_attention.out_proj.bias: shape=torch.Size([1000]), mean=0.0006, std=0.0180
2025-04-25 10:24:02,644 - DEBUG - rgb_to_depth_attention.layer_norm1.weight: shape=torch.Size([1000]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,645 - DEBUG - rgb_to_depth_attention.layer_norm1.bias: shape=torch.Size([1000]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,645 - DEBUG - rgb_to_depth_attention.layer_norm2.weight: shape=torch.Size([1000]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,645 - DEBUG - rgb_to_depth_attention.layer_norm2.bias: shape=torch.Size([1000]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,645 - DEBUG - depth_to_rgb_attention.q_proj.weight: shape=torch.Size([1000, 1000]), mean=0.0000, std=0.0183
2025-04-25 10:24:02,645 - DEBUG - depth_to_rgb_attention.q_proj.bias: shape=torch.Size([1000]), mean=-0.0002, std=0.0181
2025-04-25 10:24:02,645 - DEBUG - depth_to_rgb_attention.k_proj.weight: shape=torch.Size([1000, 1000]), mean=0.0000, std=0.0183
2025-04-25 10:24:02,645 - DEBUG - depth_to_rgb_attention.k_proj.bias: shape=torch.Size([1000]), mean=-0.0001, std=0.0179
2025-04-25 10:24:02,645 - DEBUG - depth_to_rgb_attention.v_proj.weight: shape=torch.Size([1000, 1000]), mean=-0.0000, std=0.0183
2025-04-25 10:24:02,646 - DEBUG - depth_to_rgb_attention.v_proj.bias: shape=torch.Size([1000]), mean=-0.0000, std=0.0177
2025-04-25 10:24:02,646 - DEBUG - depth_to_rgb_attention.out_proj.weight: shape=torch.Size([1000, 1000]), mean=0.0000, std=0.0183
2025-04-25 10:24:02,646 - DEBUG - depth_to_rgb_attention.out_proj.bias: shape=torch.Size([1000]), mean=0.0010, std=0.0185
2025-04-25 10:24:02,646 - DEBUG - depth_to_rgb_attention.layer_norm1.weight: shape=torch.Size([1000]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,646 - DEBUG - depth_to_rgb_attention.layer_norm1.bias: shape=torch.Size([1000]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,646 - DEBUG - depth_to_rgb_attention.layer_norm2.weight: shape=torch.Size([1000]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,646 - DEBUG - depth_to_rgb_attention.layer_norm2.bias: shape=torch.Size([1000]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,646 - DEBUG - rgb_to_rgb_down_attention.q_proj.weight: shape=torch.Size([1000, 1000]), mean=-0.0000, std=0.0183
2025-04-25 10:24:02,647 - DEBUG - rgb_to_rgb_down_attention.q_proj.bias: shape=torch.Size([1000]), mean=0.0001, std=0.0187
2025-04-25 10:24:02,647 - DEBUG - rgb_to_rgb_down_attention.k_proj.weight: shape=torch.Size([1000, 1000]), mean=0.0000, std=0.0183
2025-04-25 10:24:02,647 - DEBUG - rgb_to_rgb_down_attention.k_proj.bias: shape=torch.Size([1000]), mean=-0.0008, std=0.0178
2025-04-25 10:24:02,647 - DEBUG - rgb_to_rgb_down_attention.v_proj.weight: shape=torch.Size([1000, 1000]), mean=-0.0000, std=0.0183
2025-04-25 10:24:02,647 - DEBUG - rgb_to_rgb_down_attention.v_proj.bias: shape=torch.Size([1000]), mean=-0.0002, std=0.0180
2025-04-25 10:24:02,647 - DEBUG - rgb_to_rgb_down_attention.out_proj.weight: shape=torch.Size([1000, 1000]), mean=0.0000, std=0.0183
2025-04-25 10:24:02,647 - DEBUG - rgb_to_rgb_down_attention.out_proj.bias: shape=torch.Size([1000]), mean=-0.0002, std=0.0178
2025-04-25 10:24:02,647 - DEBUG - rgb_to_rgb_down_attention.layer_norm1.weight: shape=torch.Size([1000]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,647 - DEBUG - rgb_to_rgb_down_attention.layer_norm1.bias: shape=torch.Size([1000]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,648 - DEBUG - rgb_to_rgb_down_attention.layer_norm2.weight: shape=torch.Size([1000]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,648 - DEBUG - rgb_to_rgb_down_attention.layer_norm2.bias: shape=torch.Size([1000]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,648 - DEBUG - rgb_down_to_rgb_attention.q_proj.weight: shape=torch.Size([1000, 1000]), mean=-0.0000, std=0.0183
2025-04-25 10:24:02,648 - DEBUG - rgb_down_to_rgb_attention.q_proj.bias: shape=torch.Size([1000]), mean=0.0004, std=0.0185
2025-04-25 10:24:02,648 - DEBUG - rgb_down_to_rgb_attention.k_proj.weight: shape=torch.Size([1000, 1000]), mean=-0.0000, std=0.0183
2025-04-25 10:24:02,648 - DEBUG - rgb_down_to_rgb_attention.k_proj.bias: shape=torch.Size([1000]), mean=0.0001, std=0.0183
2025-04-25 10:24:02,648 - DEBUG - rgb_down_to_rgb_attention.v_proj.weight: shape=torch.Size([1000, 1000]), mean=-0.0000, std=0.0183
2025-04-25 10:24:02,648 - DEBUG - rgb_down_to_rgb_attention.v_proj.bias: shape=torch.Size([1000]), mean=0.0001, std=0.0182
2025-04-25 10:24:02,649 - DEBUG - rgb_down_to_rgb_attention.out_proj.weight: shape=torch.Size([1000, 1000]), mean=-0.0000, std=0.0182
2025-04-25 10:24:02,649 - DEBUG - rgb_down_to_rgb_attention.out_proj.bias: shape=torch.Size([1000]), mean=-0.0001, std=0.0178
2025-04-25 10:24:02,649 - DEBUG - rgb_down_to_rgb_attention.layer_norm1.weight: shape=torch.Size([1000]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,649 - DEBUG - rgb_down_to_rgb_attention.layer_norm1.bias: shape=torch.Size([1000]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,649 - DEBUG - rgb_down_to_rgb_attention.layer_norm2.weight: shape=torch.Size([1000]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,649 - DEBUG - rgb_down_to_rgb_attention.layer_norm2.bias: shape=torch.Size([1000]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,649 - DEBUG - fusion_layer.0.weight: shape=torch.Size([3000]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,649 - DEBUG - fusion_layer.0.bias: shape=torch.Size([3000]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,650 - DEBUG - fusion_layer.1.weight: shape=torch.Size([1000, 3000]), mean=-0.0000, std=0.0105
2025-04-25 10:24:02,650 - DEBUG - fusion_layer.1.bias: shape=torch.Size([1000]), mean=-0.0003, std=0.0106
2025-04-25 10:24:02,650 - DEBUG - fusion_layer.2.weight: shape=torch.Size([1000]), mean=1.0000, std=0.0000
2025-04-25 10:24:02,650 - DEBUG - fusion_layer.2.bias: shape=torch.Size([1000]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,650 - DEBUG - rnn.weight_ih_l0: shape=torch.Size([4000, 1000]), mean=-0.0000, std=0.0447
2025-04-25 10:24:02,650 - DEBUG - rnn.weight_hh_l0: shape=torch.Size([4000, 1000]), mean=0.0000, std=0.0447
2025-04-25 10:24:02,650 - DEBUG - rnn.bias_ih_l0: shape=torch.Size([4000]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,650 - DEBUG - rnn.bias_hh_l0: shape=torch.Size([4000]), mean=0.2500, std=0.4331
2025-04-25 10:24:02,651 - DEBUG - rnn.weight_ih_l1: shape=torch.Size([4000, 1000]), mean=-0.0001, std=0.0447
2025-04-25 10:24:02,651 - DEBUG - rnn.weight_hh_l1: shape=torch.Size([4000, 1000]), mean=-0.0000, std=0.0447
2025-04-25 10:24:02,651 - DEBUG - rnn.bias_ih_l1: shape=torch.Size([4000]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,651 - DEBUG - rnn.bias_hh_l1: shape=torch.Size([4000]), mean=0.2500, std=0.4331
2025-04-25 10:24:02,651 - DEBUG - pose_regressor.weight: shape=torch.Size([6, 1000]), mean=-0.0008, std=0.0448
2025-04-25 10:24:02,651 - DEBUG - pose_regressor.bias: shape=torch.Size([6]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,651 - DEBUG - pose_uncertainty.weight: shape=torch.Size([6, 1000]), mean=-0.0011, std=0.0446
2025-04-25 10:24:02,651 - DEBUG - pose_uncertainty.bias: shape=torch.Size([6]), mean=0.0000, std=0.0000
2025-04-25 10:24:02,652 - INFO - Testing forward pass with dummy data...
2025-04-25 10:24:02,672 - INFO - Forward pass successful. Output shape: torch.Size([2, 4, 6])
2025-04-25 10:24:02,673 - INFO - Model has 323 parameters
2025-04-25 10:24:02,673 - INFO - Sample model keys: ['rgb_encoder.conv1.0.weight', 'rgb_encoder.conv1.1.weight', 'rgb_encoder.conv1.1.bias', 'rgb_encoder.conv1.1.running_mean', 'rgb_encoder.conv1.1.running_var', 'rgb_encoder.conv1.1.num_batches_tracked', 'rgb_encoder.conv2.0.weight', 'rgb_encoder.conv2.1.weight', 'rgb_encoder.conv2.1.bias', 'rgb_encoder.conv2.1.running_mean']
2025-04-25 10:24:02,673 - INFO - Target prefixes that exist in model: []
2025-04-25 10:24:02,673 - INFO - Extended target prefixes: ['rgb_encoder.flownet', 'rgb_down_encoder.flownet', 'rgb_encoder.conv1', 'rgb_encoder.conv2', 'rgb_encoder.conv3', 'rgb_encoder.conv3_1', 'rgb_encoder.conv4', 'rgb_encoder.conv4_1', 'rgb_encoder.conv5', 'rgb_encoder.conv5_1', 'rgb_encoder.conv6', 'rgb_down_encoder.conv1', 'rgb_down_encoder.conv2', 'rgb_down_encoder.conv3', 'rgb_down_encoder.conv3_1', 'rgb_down_encoder.conv4', 'rgb_down_encoder.conv4_1', 'rgb_down_encoder.conv5', 'rgb_down_encoder.conv5_1', 'rgb_down_encoder.conv6', 'rgb_encoder_flownet', 'rgb_down_encoder_flownet']
2025-04-25 10:24:02,673 - INFO - Loading pre-trained FlowNet weights from /home/krkavinda/DeepVO-pytorch/FlowNet_models/pytorch/flownets_bn_EPE2.459.pth
2025-04-25 10:24:02,739 - INFO - FlowNet weights file contains 4 keys: ['state_dict', 'epoch', 'arch', 'best_EPE']
2025-04-25 10:24:02,740 - INFO - Found 'state_dict' key in weights file, using it for loading
2025-04-25 10:24:02,740 - INFO - Pretrained weights contain 63 parameters
2025-04-25 10:24:02,740 - INFO - Sample weight keys: ['conv1.0.weight', 'conv1.1.weight', 'conv1.1.bias', 'conv1.1.running_mean', 'conv1.1.running_var']
2025-04-25 10:24:02,741 - INFO - No key matches found, attempting shape-based matching...
2025-04-25 10:24:02,741 - INFO - Matched 49 layers by shape
2025-04-25 10:24:02,744 - INFO - Successfully loaded 49 layers from FlowNet weights
2025-04-25 10:24:02,744 - INFO - 
==== VALIDATING DATALOADERS ====
2025-04-25 10:24:02,744 - INFO - Checking Training dataloader...
2025-04-25 10:24:21,528 - INFO - Training batch sequence lengths: [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]
2025-04-25 10:24:21,528 - INFO - Training RGB shape: torch.Size([24, 7, 3, 184, 608])
2025-04-25 10:24:21,542 - INFO - Training RGB stats: min=-4.2093, max=0.5773, mean=-1.9682
2025-04-25 10:24:21,569 - INFO - Training RGB Down shape: torch.Size([24, 7, 3, 184, 608])
2025-04-25 10:24:21,583 - INFO - Training RGB Down stats: min=-3.9362, max=1.4494, mean=-2.2041
2025-04-25 10:24:21,601 - INFO - Training Ground truth shape: torch.Size([24, 6, 6])
2025-04-25 10:24:21,604 - INFO - Training Ground truth stats: min=-0.6153, max=0.8049, mean=0.0255
2025-04-25 10:24:21,605 - INFO - Training Trajectory IDs: [['Kite_training/sunny', 'trajectory_0003'], ['Kite_training/sunny', 'trajectory_0007'], ['Kite_training/sunny', 'trajectory_0004']]
2025-04-25 10:24:21,904 - INFO - Training forward pass successful. Output shape: torch.Size([24, 6, 6])
2025-04-25 10:24:21,904 - INFO - Training loss calculation successful. Loss: -9.453268
2025-04-25 10:24:21,905 - INFO - Checking Validation dataloader...
2025-04-25 10:24:35,492 - INFO - Validation batch sequence lengths: [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]
2025-04-25 10:24:35,492 - INFO - Validation RGB shape: torch.Size([24, 5, 3, 184, 608])
2025-04-25 10:24:35,502 - INFO - Validation RGB stats: min=-4.2093, max=0.5773, mean=-2.0273
2025-04-25 10:24:35,517 - INFO - Validation RGB Down shape: torch.Size([24, 5, 3, 184, 608])
2025-04-25 10:24:35,527 - INFO - Validation RGB Down stats: min=-3.9362, max=1.4292, mean=-1.9846
2025-04-25 10:24:35,541 - INFO - Validation Ground truth shape: torch.Size([24, 4, 6])
2025-04-25 10:24:35,542 - INFO - Validation Ground truth stats: min=-0.6255, max=0.7023, mean=0.0116
2025-04-25 10:24:35,542 - INFO - Validation Trajectory IDs: [['Kite_training/sunny', 'trajectory_0013'], ['Kite_training/sunny', 'trajectory_0008'], ['Kite_training/sunny', 'trajectory_0014']]
2025-04-25 10:24:35,707 - INFO - Validation forward pass successful. Output shape: torch.Size([24, 4, 6])
2025-04-25 10:24:35,707 - INFO - Validation loss calculation successful. Loss: -13.191113
2025-04-25 10:24:35,707 - INFO - 
==== VALIDATING LOSS CALCULATION ====
2025-04-25 10:24:42,721 - INFO - Batch 0: Model loss = -21.452110, Manual loss = 2.717883
2025-04-25 10:24:42,724 - INFO -   Difference: 24.169993
2025-04-25 10:24:46,518 - INFO - Batch 1: Model loss = -21.029636, Manual loss = 2.758513
2025-04-25 10:24:46,519 - INFO -   Difference: 23.788150
2025-04-25 10:24:46,716 - INFO - Batch 2: Model loss = -20.101986, Manual loss = 3.345857
2025-04-25 10:24:46,717 - INFO -   Difference: 23.447843
2025-04-25 10:24:46,894 - INFO - Batch 3: Model loss = -21.233868, Manual loss = 2.871778
2025-04-25 10:24:46,894 - INFO -   Difference: 24.105646
2025-04-25 10:24:47,069 - INFO - Batch 4: Model loss = -20.052086, Manual loss = 3.007031
2025-04-25 10:24:47,069 - INFO -   Difference: 23.059117
2025-04-25 10:24:49,457 - INFO - Validated 5 batches.
2025-04-25 10:24:49,457 - INFO - Average validation loss: -20.773937
2025-04-25 10:24:49,457 - INFO - Min: -21.452110, Max: -20.052086
2025-04-25 10:24:49,457 - INFO - Starting training for 100 epochs
2025-04-25 10:24:59,509 - INFO - 
TRAINING BATCH DETAILS:
2025-04-25 10:24:59,512 - INFO - Sequence lengths: [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]
2025-04-25 10:24:59,513 - INFO - rgb tensor shape: torch.Size([24, 7, 3, 184, 608])
2025-04-25 10:24:59,516 - INFO - rgb_down tensor shape: torch.Size([24, 7, 3, 184, 608])
2025-04-25 10:24:59,519 - INFO - Ground truth shape: torch.Size([24, 6, 6])
2025-04-25 10:24:59,618 - INFO - Ground truth sample (first item, first 3 frames): tensor([[-1.0096e-03, -9.7106e-05, -7.4541e-05,  2.3395e-01, -1.2176e-01,
          4.0495e-02],
        [-8.7128e-04, -2.3060e-05, -3.8123e-05,  2.3470e-01, -1.2453e-01,
          4.3899e-02],
        [-7.2484e-04,  6.3254e-05, -1.3954e-06,  2.3544e-01, -1.2734e-01,
          4.7216e-02]], device='cuda:0')
2025-04-25 10:27:52,858 - INFO - 
VALIDATION BATCH DETAILS:
2025-04-25 10:27:52,862 - INFO - Sequence lengths: [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]
2025-04-25 10:27:52,865 - INFO - rgb tensor shape: torch.Size([24, 5, 3, 184, 608])
2025-04-25 10:27:52,868 - INFO - rgb_down tensor shape: torch.Size([24, 5, 3, 184, 608])
2025-04-25 10:27:52,871 - INFO - Ground truth shape: torch.Size([24, 4, 6])
2025-04-25 10:27:52,876 - INFO - Ground truth sample (first item, first 3 frames): tensor([[ 7.6753e-04, -4.5852e-04, -8.2848e-03, -1.0227e-02, -2.8081e-01,
         -1.6335e-02],
        [ 7.0024e-04,  1.5281e-04, -8.7806e-03, -1.0760e-02, -2.8109e-01,
         -1.3576e-02],
        [ 7.0262e-04,  8.9175e-04, -9.6634e-03, -1.1232e-02, -2.8142e-01,
         -1.0409e-02]], device='cuda:0')
2025-04-25 10:28:32,469 - INFO - Epoch 1/100, Train Loss: -83.064732, Valid Loss: -154.387482, ETA: 6:07:58
2025-04-25 10:28:35,052 - INFO - New best validation loss: -154.387482
2025-04-25 10:28:37,677 - INFO - New best training loss: -83.064732
2025-04-25 10:28:53,247 - INFO - 
TRAINING BATCH DETAILS:
2025-04-25 10:28:53,249 - INFO - Sequence lengths: [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]
2025-04-25 10:28:53,249 - INFO - rgb tensor shape: torch.Size([24, 7, 3, 184, 608])
2025-04-25 10:28:53,251 - INFO - rgb_down tensor shape: torch.Size([24, 7, 3, 184, 608])
2025-04-25 10:28:53,254 - INFO - Ground truth shape: torch.Size([24, 6, 6])
2025-04-25 10:28:53,257 - INFO - Ground truth sample (first item, first 3 frames): tensor([[ 0.0030,  0.0006, -0.0169,  0.3189, -0.2591,  0.0216],
        [ 0.0029,  0.0008, -0.0168,  0.3187, -0.2544,  0.0212],
        [ 0.0029,  0.0010, -0.0168,  0.3184, -0.2499,  0.0208]],
       device='cuda:0')
2025-04-25 10:31:47,139 - INFO - 
VALIDATION BATCH DETAILS:
2025-04-25 10:31:47,142 - INFO - Sequence lengths: [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]
2025-04-25 10:31:47,143 - INFO - rgb tensor shape: torch.Size([24, 5, 3, 184, 608])
2025-04-25 10:31:47,144 - INFO - rgb_down tensor shape: torch.Size([24, 5, 3, 184, 608])
2025-04-25 10:31:47,146 - INFO - Ground truth shape: torch.Size([24, 4, 6])
2025-04-25 10:31:47,149 - INFO - Ground truth sample (first item, first 3 frames): tensor([[ 9.6865e-04, -4.8339e-04, -3.8772e-03, -5.4990e-01, -6.0167e-02,
          2.8226e-01],
        [ 1.0845e-03, -4.2622e-04, -4.1370e-03, -5.4943e-01, -6.1862e-02,
          2.8314e-01],
        [ 1.1823e-03, -3.6572e-04, -4.4012e-03, -5.4898e-01, -6.3656e-02,
          2.8385e-01]], device='cuda:0')
2025-04-25 10:32:28,476 - INFO - Epoch 2/100, Train Loss: -136.146810, Valid Loss: -186.980615, ETA: 6:10:36
2025-04-25 10:32:31,052 - INFO - New best validation loss: -186.980615
2025-04-25 10:32:33,601 - INFO - New best training loss: -136.146810
2025-04-25 10:32:50,161 - INFO - 
TRAINING BATCH DETAILS:
2025-04-25 10:32:50,165 - INFO - Sequence lengths: [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]
2025-04-25 10:32:50,165 - INFO - rgb tensor shape: torch.Size([24, 7, 3, 184, 608])
2025-04-25 10:32:50,168 - INFO - rgb_down tensor shape: torch.Size([24, 7, 3, 184, 608])
2025-04-25 10:32:50,170 - INFO - Ground truth shape: torch.Size([24, 6, 6])
2025-04-25 10:32:50,174 - INFO - Ground truth sample (first item, first 3 frames): tensor([[ 5.9709e-04, -8.8352e-04,  2.9508e-05,  6.6737e-01,  3.8043e-02,
         -1.0324e-01],
        [ 9.4161e-04, -7.6690e-04,  1.7711e-04,  6.6631e-01,  3.8087e-02,
         -1.0023e-01],
        [ 1.3488e-03, -6.1045e-04,  3.5629e-04,  6.6527e-01,  3.8021e-02,
         -9.7126e-02]], device='cuda:0')
2025-04-25 10:35:43,709 - INFO - 
VALIDATION BATCH DETAILS:
2025-04-25 10:35:43,712 - INFO - Sequence lengths: [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]
2025-04-25 10:35:43,712 - INFO - rgb tensor shape: torch.Size([24, 5, 3, 184, 608])
2025-04-25 10:35:43,714 - INFO - rgb_down tensor shape: torch.Size([24, 5, 3, 184, 608])
2025-04-25 10:35:43,715 - INFO - Ground truth shape: torch.Size([24, 4, 6])
2025-04-25 10:35:43,720 - INFO - Ground truth sample (first item, first 3 frames): tensor([[ 6.5925e-04,  1.0009e-03,  6.5668e-05,  4.4044e-01, -3.9112e-01,
         -1.1999e-01],
        [ 1.0507e-03,  8.3576e-04,  9.4256e-05,  4.4051e-01, -3.9314e-01,
         -1.1949e-01],
        [ 1.4834e-03,  6.7240e-04,  1.2552e-04,  4.4052e-01, -3.9521e-01,
         -1.1894e-01]], device='cuda:0')
2025-04-25 10:36:24,163 - INFO - Epoch 3/100, Train Loss: -164.293064, Valid Loss: -206.171025, ETA: 6:08:47
2025-04-25 10:36:26,760 - INFO - New best validation loss: -206.171025
2025-04-25 10:36:29,307 - INFO - New best training loss: -164.293064
2025-04-25 10:36:41,689 - INFO - 
TRAINING BATCH DETAILS:
2025-04-25 10:36:41,692 - INFO - Sequence lengths: [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]
2025-04-25 10:36:41,693 - INFO - rgb tensor shape: torch.Size([24, 7, 3, 184, 608])
2025-04-25 10:36:41,695 - INFO - rgb_down tensor shape: torch.Size([24, 7, 3, 184, 608])
2025-04-25 10:36:41,698 - INFO - Ground truth shape: torch.Size([24, 6, 6])
2025-04-25 10:36:41,702 - INFO - Ground truth sample (first item, first 3 frames): tensor([[ 0.0019,  0.0019,  0.0027,  0.5468, -0.1023, -0.1152],
        [ 0.0012,  0.0020,  0.0022,  0.5483, -0.1007, -0.1140],
        [ 0.0008,  0.0020,  0.0018,  0.5498, -0.0988, -0.1130]],
       device='cuda:0')
2025-04-25 10:39:36,844 - INFO - 
VALIDATION BATCH DETAILS:
2025-04-25 10:39:36,849 - INFO - Sequence lengths: [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]
2025-04-25 10:39:36,851 - INFO - rgb tensor shape: torch.Size([24, 5, 3, 184, 608])
2025-04-25 10:39:36,854 - INFO - rgb_down tensor shape: torch.Size([24, 5, 3, 184, 608])
2025-04-25 10:39:36,859 - INFO - Ground truth shape: torch.Size([24, 4, 6])
2025-04-25 10:39:36,866 - INFO - Ground truth sample (first item, first 3 frames): tensor([[ 0.0032,  0.0051,  0.0141,  0.6174,  0.0165, -0.0030],
        [ 0.0031,  0.0049,  0.0136,  0.6167,  0.0115,  0.0021],
        [ 0.0032,  0.0047,  0.0129,  0.6161,  0.0068,  0.0074]],
       device='cuda:0')
2025-04-25 10:40:20,072 - INFO - Epoch 4/100, Train Loss: -183.458098, Valid Loss: -224.167917, ETA: 6:06:03
2025-04-25 10:40:22,694 - INFO - New best validation loss: -224.167917
2025-04-25 10:40:25,280 - INFO - New best training loss: -183.458098
2025-04-25 10:40:41,982 - INFO - 
TRAINING BATCH DETAILS:
2025-04-25 10:40:41,983 - INFO - Sequence lengths: [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]
2025-04-25 10:40:41,983 - INFO - rgb tensor shape: torch.Size([24, 7, 3, 184, 608])
2025-04-25 10:40:41,986 - INFO - rgb_down tensor shape: torch.Size([24, 7, 3, 184, 608])
2025-04-25 10:40:41,988 - INFO - Ground truth shape: torch.Size([24, 6, 6])
2025-04-25 10:40:41,991 - INFO - Ground truth sample (first item, first 3 frames): tensor([[-3.1939e-03,  2.7109e-03, -2.7150e-04,  2.8957e-01, -9.5017e-02,
          1.0624e-01],
        [-2.9990e-03,  3.2589e-03, -1.0805e-03,  2.8657e-01, -9.8021e-02,
          1.0504e-01],
        [-2.7681e-03,  3.7441e-03, -2.0550e-03,  2.8349e-01, -1.0081e-01,
          1.0427e-01]], device='cuda:0')
2025-04-25 10:43:36,111 - INFO - 
VALIDATION BATCH DETAILS:
2025-04-25 10:43:36,114 - INFO - Sequence lengths: [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]
2025-04-25 10:43:36,114 - INFO - rgb tensor shape: torch.Size([24, 5, 3, 184, 608])
2025-04-25 10:43:36,116 - INFO - rgb_down tensor shape: torch.Size([24, 5, 3, 184, 608])
2025-04-25 10:43:36,117 - INFO - Ground truth shape: torch.Size([24, 4, 6])
2025-04-25 10:43:36,121 - INFO - Ground truth sample (first item, first 3 frames): tensor([[ 4.4409e-03,  2.8589e-04,  2.8927e-05,  5.1851e-03, -5.5791e-01,
         -9.9336e-02],
        [ 4.6205e-03,  2.9512e-04,  2.6769e-05,  5.1190e-03, -5.5705e-01,
         -9.8424e-02],
        [ 4.7235e-03,  2.9992e-04,  2.4214e-05,  5.0511e-03, -5.5614e-01,
         -9.7431e-02]], device='cuda:0')
2025-04-25 10:44:15,503 - INFO - Epoch 5/100, Train Loss: -199.699792, Valid Loss: -238.997088, ETA: 6:02:41
2025-04-25 10:44:18,081 - INFO - New best validation loss: -238.997088
2025-04-25 10:44:20,691 - INFO - New best training loss: -199.699792
2025-04-25 10:44:22,539 - INFO - Saved checkpoint at epoch 5
2025-04-25 10:44:35,108 - INFO - 
TRAINING BATCH DETAILS:
2025-04-25 10:44:35,111 - INFO - Sequence lengths: [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]
2025-04-25 10:44:35,112 - INFO - rgb tensor shape: torch.Size([24, 7, 3, 184, 608])
2025-04-25 10:44:35,115 - INFO - rgb_down tensor shape: torch.Size([24, 7, 3, 184, 608])
2025-04-25 10:44:35,117 - INFO - Ground truth shape: torch.Size([24, 6, 6])
2025-04-25 10:44:35,120 - INFO - Ground truth sample (first item, first 3 frames): tensor([[ 2.2924e-03, -8.3632e-04,  2.3080e-04,  2.4697e-01,  1.6782e-02,
         -1.3780e-02],
        [ 1.8999e-03, -5.8012e-04,  2.0325e-04,  2.4780e-01,  1.6395e-02,
         -1.1758e-02],
        [ 1.6042e-03, -3.3306e-04,  2.0394e-04,  2.4862e-01,  1.6073e-02,
         -9.6404e-03]], device='cuda:0')
2025-04-25 10:47:24,783 - INFO - 
VALIDATION BATCH DETAILS:
2025-04-25 10:47:24,786 - INFO - Sequence lengths: [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]
2025-04-25 10:47:24,786 - INFO - rgb tensor shape: torch.Size([24, 5, 3, 184, 608])
2025-04-25 10:47:24,788 - INFO - rgb_down tensor shape: torch.Size([24, 5, 3, 184, 608])
2025-04-25 10:47:24,789 - INFO - Ground truth shape: torch.Size([24, 4, 6])
2025-04-25 10:47:24,792 - INFO - Ground truth sample (first item, first 3 frames): tensor([[ 0.0004,  0.0017, -0.0081,  0.3633, -0.1202,  0.1114],
        [ 0.0005,  0.0020, -0.0083,  0.3631, -0.1194,  0.1125],
        [ 0.0005,  0.0022, -0.0085,  0.3629, -0.1186,  0.1137]],
       device='cuda:0')
2025-04-25 10:48:06,878 - INFO - Epoch 6/100, Train Loss: -214.703456, Valid Loss: -253.900296, ETA: 5:59:16
2025-04-25 10:48:09,432 - INFO - New best validation loss: -253.900296
2025-04-25 10:48:12,125 - INFO - New best training loss: -214.703456
2025-04-25 10:48:23,619 - INFO - 
TRAINING BATCH DETAILS:
2025-04-25 10:48:23,621 - INFO - Sequence lengths: [7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]
2025-04-25 10:48:23,622 - INFO - rgb tensor shape: torch.Size([24, 7, 3, 184, 608])
2025-04-25 10:48:23,625 - INFO - rgb_down tensor shape: torch.Size([24, 7, 3, 184, 608])
2025-04-25 10:48:23,627 - INFO - Ground truth shape: torch.Size([24, 6, 6])
2025-04-25 10:48:23,630 - INFO - Ground truth sample (first item, first 3 frames): tensor([[-0.0042,  0.0039, -0.0122,  0.4288,  0.3579, -0.0521],
        [-0.0040,  0.0042, -0.0114,  0.4262,  0.3596, -0.0523],
        [-0.0038,  0.0045, -0.0106,  0.4239,  0.3609, -0.0525]],
       device='cuda:0')
